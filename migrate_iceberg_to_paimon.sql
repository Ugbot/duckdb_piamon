-- Iceberg to Paimon Migration SQL
-- Generated by migrate_iceberg_to_paimon.py

-- Load our dual-format extension (supports both Iceberg AND Paimon)
    LOAD 'paimon.duckdb_extension';

    -- The extension automatically loads parquet and avro dependencies
    -- No need to manually load them separately

-- Create Paimon warehouse (filesystem-based)
    ATTACH 'file://paimon_data' AS paimon_db (
        TYPE PAIMON,
        -- Optional: Add catalog configuration here
        -- ENDPOINT 'http://localhost:8080'  -- for REST catalog
        -- ACCESS_KEY 'key', SECRET_KEY 'secret'  -- for S3
    );

    -- Create table with same schema as Iceberg data
    -- PRIMARY KEY NOT ENFORCED allows Paimon to manage the key
    CREATE TABLE paimon_db.default.user_events (
        id INTEGER PRIMARY KEY NOT ENFORCED,
        name VARCHAR,
        event_timestamp TIMESTAMP
    );

    -- Optional: Add partitioning for better performance
    -- PARTITIONED BY (event_date DATE)  -- if we derived date from timestamp

-- Migration query:
INSERT INTO paimon_db.default.user_events
SELECT * FROM iceberg_scan('data/persistent/big_query_error');

-- Read back data from Paimon table
    SELECT * FROM paimon_scan('file://paimon_data/default/user_events/')
    ORDER BY id;

    -- Check snapshots (Paimon's version control)
    SELECT snapshot_id, sequence_number, timestamp_ms, manifest_list
    FROM paimon_snapshots('file://paimon_data/default/user_events/')
    ORDER BY snapshot_id DESC
    LIMIT 1;

    -- Verify data integrity
    SELECT
        COUNT(*) as total_rows,
        COUNT(DISTINCT id) as unique_ids,
        MIN(event_timestamp) as earliest_event,
        MAX(event_timestamp) as latest_event
    FROM paimon_scan('file://paimon_data/default/user_events/');
